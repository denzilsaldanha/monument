{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denzilsaldanha/monument/blob/master/Cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm4lAYuzGsLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgkqEFIgGpj7",
        "colab_type": "code",
        "outputId": "b53ae6f3-6cd5-4f0e-d6ea-8d7e51a2977a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "count = 0\n",
        "for folder in os.listdir('drive/My Drive/train_sample_images/'):\n",
        "    for file in os.listdir('drive/My Drive/train_sample_images/'+str(folder)):\n",
        "        if file.endswith('.jpg') or file.endswith('.mpo'):\n",
        "            try:\n",
        "                img = Image.open('drive/My Drive/train_sample_images/'+str(folder)+'/'+str(file)) # open the image file\n",
        "                img.verify() # verify that it is, in fact an image\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                count +=1\n",
        "                os.remove('drive/My Drive/train_sample_images/'+str(folder)+'/'+str(file))\n",
        "                print('Bad file: ', str(file))\n",
        "                \n",
        "                \n",
        "print(\"CHECKED ALL FILES\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/JpegImagePlugin.py:775: UserWarning: Image appears to be a malformed MPO file, it will be interpreted as a base JPEG file\n",
            "  warnings.warn(\"Image appears to be a malformed MPO file, it will be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CHECKED ALL FILES\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jygSoUTC_1D3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "# Installing Theano\n",
        "# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
        "\n",
        "# Installing Tensorflow\n",
        "# pip install tensorflow\n",
        "\n",
        "# Installing Keras\n",
        "# pip install --upgrade keras\n",
        "\n",
        "\n",
        "## WHEN UPLOADING THE DATA SET CREATE A FOLDER CALLED DATASET inside which the training and testing folder must be. Change the names accordingy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Part 1 - Building the CNN\n",
        "from keras.callbacks import CSVLogger\n",
        "import os\n",
        "import glob\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Initialising the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "classifier.add(Conv2D(32, (5,5), input_shape = (224, 224, 3), activation = 'relu')) ## Need to change the input shape\n",
        "\n",
        "# Step 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adding a second convolutional layer\n",
        "classifier.add(Conv2D(64, (3,3), activation = 'relu'))  ## Change the feature number of features to be used and the feature detector size\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "classifier.add(Conv2D(128, (3,3), activation = 'relu'))  ## Change the feature number of features to be used and the feature detector size\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(units = 512, activation = 'relu'))\n",
        "classifier.add(Dense(units = 150, activation = 'softmax')) ## CHANGE TO SOFTMAX\n",
        "\n",
        "\n",
        "\n",
        "epochs_run = 0\n",
        "\n",
        "##TO DO : Write a for loop to iterate to the latest version of the weights available.\n",
        "# if os.path.isfile(\"drive/My Drive/dataset/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "#   classifier.load_weights(\"drive/My Drive/dataset/weights.best.hdf5\")\n",
        "\n",
        "if glob.glob('drive/My Drive/GOOGLE_MONUMENT/weights-improvement-[0-9][0-9]*'):\n",
        "  weight_to_be_loaded = max(glob.glob('drive/My Drive/GOOGLE_MONUMENT/weights-improvement-[0-9][0-9]*'))\n",
        "  classifier.load_weights(weight_to_be_loaded)\n",
        "  epochs_run = int(weight_to_be_loaded[43:45])\n",
        "\n",
        "# Compiling the CNN\n",
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])## CHANGE THE LOSS\n",
        "\n",
        "# Part 2 - Fitting the CNN to the images\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True,\n",
        "                                   validation_split=0.1)\n",
        "\n",
        "# test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "                  \n",
        "                  ##CHANGE THE CLASS MODE\n",
        "training_set = train_datagen.flow_from_directory('drive/My Drive/GOOGLE_MONUMENT/train_sample_images',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 128,\n",
        "                                                 class_mode = 'categorical',\n",
        "                                                subset ='validation')\n",
        "validation_set = train_datagen.flow_from_directory(\n",
        "    'drive/My Drive/GOOGLE_MONUMENT/train_sample_images', # same directory as training data\n",
        "    target_size=(224,224),\n",
        "    batch_size=128,\n",
        "    class_mode='categorical',\n",
        "    subset='validation')\n",
        "\n",
        "# test_set = test_datagen.flow_from_directory('drive/My Drive/dataset/test_set',\n",
        "#                                             target_size = (64, 64),\n",
        "#                                             batch_size = 32,\n",
        "#                                             class_mode = 'binary')\n",
        "\n",
        "csv_file = 'drive/My Drive/GOOGLE_MONUMENT/log_starting_from_epoch_'+str(epochs_run)+'.csv'\n",
        "checkpoint=\"drive/My Drive/GOOGLE_MONUMENT/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint, monitor='val_acc', verbose=1, mode='max')\n",
        "csv_logger = CSVLogger(csv_file, append=True, separator=';')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss',verbose = 1)\n",
        "callbacks_list = [checkpoint,csv_logger,es]\n",
        "\n",
        "\n",
        "epoch_count_to_be_run = 25 \n",
        "step_size_train=training_set.n//training_set.batch_size\n",
        "classifier.summary()\n",
        "val_steps = validation_set.n//validation_set.batch_size\n",
        "classifier.fit_generator(training_set,\n",
        "                         steps_per_epoch = step_size_train ,\n",
        "                         epochs = epoch_count_to_be_run,\n",
        "                         validation_data = validation_set,\n",
        "                         validation_steps=val_steps,callbacks =callbacks_list,initial_epoch = epochs_run)\n",
        "\n",
        "\n",
        "print(\"%s: %.2f%%\" % (classifier.metrics_names[1], scores[1]*100))\n",
        " \n",
        "# serialize model to JSON\n",
        "model_json = classifier.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "classifier.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "\n",
        "# scores = classifier.evaluate(training_set, test_set, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QFAfoWaGl2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}