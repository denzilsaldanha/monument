{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vgg19-TL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denzilsaldanha/monument/blob/master/Vgg19_TL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aKYm_uzQl-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JltoNVw4QpK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dense,GlobalAveragePooling2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import CSVLogger\n",
        "from keras.applications.vgg19 import VGG19, preprocess_input\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "base_model=VGG19(weights='imagenet',include_top=False,input_shape=(224,224,3) )\n",
        "\n",
        "x=base_model.output\n",
        "\n",
        "# x = Flatten()(x)\n",
        "# x = Dense(1024, activation=\"relu\")(x)\n",
        "# x = Dropout(0.5)(x)\n",
        "# x = Dense(1024, activation=\"relu\")(x)\n",
        "# predictions = Dense(150, activation=\"softmax\")(x)\n",
        "x=GlobalAveragePooling2D()(x)\n",
        "x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
        "x=Dense(1024,activation='relu')(x) #dense layer 2\n",
        "x=Dense(512,activation='relu')(x) #dense layer 3\n",
        "preds=Dense(150,activation='softmax')(x) #final layer with softmax activation\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)\n",
        "#specify the inputs\n",
        "#specify the outputs\n",
        "#now a model has been created based on our architecture\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "layers_to_be_trained = 0 \n",
        "for layer in model.layers[:22]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[22:]:\n",
        "    layer.trainable=True\n",
        "    layers_to_be_trained +=1\n",
        "\n",
        "print(layers_to_be_trained)\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "# train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,validation_split=0.2)\n",
        "# train_generator=train_datagen.flow_from_directory('./train/', # this is where you specify the path to the main data folder\n",
        "#                                                  target_size=(224,224),\n",
        "#                                                  color_mode='rgb',\n",
        "#                                                  batch_size=32,\n",
        "#                                                  class_mode='categorical',\n",
        "#                                                  shuffle=True)\n",
        "\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('drive/My Drive/GOOGLE_MONUMENT/train_sample_images',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 128,\n",
        "                                                 class_mode = 'categorical',\n",
        "                                                subset ='training')\n",
        "validation_set = train_datagen.flow_from_directory(\n",
        "    'drive/My Drive/GOOGLE_MONUMENT/train_sample_images', # same directory as training data\n",
        "    target_size=(224,224),\n",
        "    batch_size=128,\n",
        "    class_mode='categorical',\n",
        "    subset='validation')\n",
        "\n",
        "# In[33]:\n",
        "epochs_run = 0\n",
        "\n",
        "if glob.glob('drive/My Drive/GOOGLE_MONUMENT/weights-improvement-vgg-[0-9][0-9]*'):\n",
        "  weight_to_be_loaded = max(glob.glob('drive/My Drive/GOOGLE_MONUMENT/weights-improvement-vgg-net-[0-9][0-9]*'))\n",
        "  model.load_weights(weight_to_be_loaded)\n",
        "  epochs_run = int(weight_to_be_loaded[55:57])\n",
        "\n",
        "model.compile(optimizer= 'Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "\n",
        "csv_file = 'drive/My Drive/GOOGLE_MONUMENT/log_for_vgg_net_starting_from_epoch_'+str(epochs_run)+'.csv'\n",
        "checkpoint=\"drive/My Drive/GOOGLE_MONUMENT/weights-improvement-vgg-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint, monitor='val_acc', verbose=1, mode='max')\n",
        "csv_logger = CSVLogger(csv_file, append=True, separator=';')\n",
        "callbacks_list = [csv_logger,checkpoint]\n",
        "\n",
        "#model.summary()\n",
        "step_size_train=training_set.n//training_set.batch_size\n",
        "val_steps = validation_set.n//validation_set.batch_size\n",
        "#print(val_steps)\n",
        "model.fit_generator(generator=training_set,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=5,\n",
        "                    validation_data = validation_set,validation_steps=val_steps,\n",
        "                   callbacks =callbacks_list,initial_epoch = epochs_run)\n",
        "\n",
        "\n",
        "#print(\"%s: %.2f%%\" % (classifier.metrics_names[1], scores[1]*100))\n",
        "model.save('drive/My Drive/GOOGLE_MONUMENT/model_vgg.hdf5')\n",
        "model.save_weights(\"drive/My Drive/GOOGLE_MONUMENT/model_h5_vgg.h5\")\n",
        "print(\"Saved model to disk\") \n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"drive/My Drive/GOOGLE_MONUMENT/model_vgg.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWYaXuNdTglq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}